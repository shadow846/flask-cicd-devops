
==> Audit <==
|---------|----------------------------------------------------------------------|----------|---------|---------|---------------------|---------------------|
| Command |                                 Args                                 | Profile  |  User   | Version |     Start Time      |      End Time       |
|---------|----------------------------------------------------------------------|----------|---------|---------|---------------------|---------------------|
| start   | --driver=docker                                                      | minikube | goutham | v1.36.0 | 16 Jul 25 11:28 UTC | 16 Jul 25 11:31 UTC |
| ip      |                                                                      | minikube | goutham | v1.36.0 | 16 Jul 25 11:35 UTC | 16 Jul 25 11:35 UTC |
| service | helloworld-service                                                   | minikube | goutham | v1.36.0 | 16 Jul 25 11:40 UTC | 16 Jul 25 11:40 UTC |
| service | helloworld-service                                                   | minikube | goutham | v1.36.0 | 16 Jul 25 11:46 UTC | 16 Jul 25 11:47 UTC |
| delete  |                                                                      | minikube | goutham | v1.36.0 | 16 Jul 25 11:54 UTC | 16 Jul 25 11:54 UTC |
| delete  | --all                                                                | minikube | goutham | v1.36.0 | 16 Jul 25 11:57 UTC | 16 Jul 25 11:57 UTC |
| start   | --driver=docker                                                      | minikube | goutham | v1.36.0 | 16 Jul 25 12:11 UTC |                     |
| start   | --driver=docker                                                      | minikube | goutham | v1.36.0 | 16 Jul 25 12:12 UTC |                     |
| start   | --driver=docker                                                      | minikube | goutham | v1.36.0 | 16 Jul 25 12:13 UTC | 16 Jul 25 12:14 UTC |
| kubectl | -- get nodes                                                         | minikube | goutham | v1.36.0 | 16 Jul 25 12:17 UTC | 16 Jul 25 12:17 UTC |
| kubectl | -- create namespace monitoring                                       | minikube | goutham | v1.36.0 | 16 Jul 25 12:17 UTC | 16 Jul 25 12:17 UTC |
| kubectl | -- --namespace monitoring get                                        | minikube | goutham | v1.36.0 | 16 Jul 25 12:18 UTC | 16 Jul 25 12:18 UTC |
|         | secrets prom-stack-grafana -o                                        |          |         |         |                     |                     |
|         | jsonpath={.data.admin-password}                                      |          |         |         |                     |                     |
| kubectl | -- --namespace monitoring get pod -l                                 | minikube | goutham | v1.36.0 | 16 Jul 25 12:18 UTC | 16 Jul 25 12:18 UTC |
|         | app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prom-stack |          |         |         |                     |                     |
|         | -oname                                                               |          |         |         |                     |                     |
| kubectl | -- --namespace monitoring port-forward                               | minikube | goutham | v1.36.0 | 16 Jul 25 12:18 UTC |                     |
|         | pod/prom-stack-grafana-549f5d664-vv6bk                               |          |         |         |                     |                     |
|         |                                                                 3000 |          |         |         |                     |                     |
| kubectl | -- --namespace monitoring get pod -l                                 | minikube | goutham | v1.36.0 | 16 Jul 25 12:19 UTC | 16 Jul 25 12:19 UTC |
|         | app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prom-stack |          |         |         |                     |                     |
|         | -oname                                                               |          |         |         |                     |                     |
| kubectl | -- --namespace monitoring port-forward                               | minikube | goutham | v1.36.0 | 16 Jul 25 12:19 UTC |                     |
|         | pod/prom-stack-grafana-549f5d664-vv6bk                               |          |         |         |                     |                     |
|         |                                                                 3000 |          |         |         |                     |                     |
| kubectl | -- get pods -n monitoring                                            | minikube | goutham | v1.36.0 | 16 Jul 25 12:19 UTC | 16 Jul 25 12:19 UTC |
| kubectl | -- get pods -n monitoring                                            | minikube | goutham | v1.36.0 | 16 Jul 25 12:21 UTC | 16 Jul 25 12:21 UTC |
| kubectl | -- --namespace monitoring get pod -l                                 | minikube | goutham | v1.36.0 | 16 Jul 25 12:22 UTC | 16 Jul 25 12:22 UTC |
|         | app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prom-stack |          |         |         |                     |                     |
|         | -oname                                                               |          |         |         |                     |                     |
| kubectl | -- --namespace monitoring port-forward                               | minikube | goutham | v1.36.0 | 16 Jul 25 12:22 UTC |                     |
|         | pod/prom-stack-grafana-549f5d664-vv6bk                               |          |         |         |                     |                     |
|         |                                                                 3000 |          |         |         |                     |                     |
| start   |                                                                      | minikube | goutham | v1.36.0 | 17 Jul 25 11:23 UTC | 17 Jul 25 11:23 UTC |
| kubectl | -- get nodes                                                         | minikube | goutham | v1.36.0 | 17 Jul 25 11:25 UTC | 17 Jul 25 11:25 UTC |
| tunnel  |                                                                      | minikube | goutham | v1.36.0 | 17 Jul 25 11:42 UTC | 17 Jul 25 11:44 UTC |
| image   | load cloudblade-barista:latest                                       | minikube | goutham | v1.36.0 | 17 Jul 25 11:50 UTC | 17 Jul 25 11:50 UTC |
| service | cloudblade-service --url                                             | minikube | goutham | v1.36.0 | 17 Jul 25 11:52 UTC | 17 Jul 25 11:52 UTC |
| service | cloubblade-webops --url                                              | minikube | goutham | v1.36.0 | 17 Jul 25 12:11 UTC |                     |
| service | cloudblade-webops --url                                              | minikube | goutham | v1.36.0 | 17 Jul 25 12:12 UTC |                     |
| delete  |                                                                      | minikube | goutham | v1.36.0 | 17 Jul 25 12:13 UTC | 17 Jul 25 12:13 UTC |
| start   |                                                                      | minikube | goutham | v1.36.0 | 17 Jul 25 12:13 UTC | 17 Jul 25 12:15 UTC |
| image   | load cloudblade-barista:latest                                       | minikube | goutham | v1.36.0 | 17 Jul 25 12:15 UTC | 17 Jul 25 12:15 UTC |
| service | cloudblade-service --url                                             | minikube | goutham | v1.36.0 | 17 Jul 25 12:16 UTC | 17 Jul 25 12:18 UTC |
| image   | load cloudblade-barista:latest                                       | minikube | goutham | v1.36.0 | 17 Jul 25 12:29 UTC | 17 Jul 25 12:29 UTC |
| service | cloudblade-service --url                                             | minikube | goutham | v1.36.0 | 17 Jul 25 12:29 UTC | 17 Jul 25 12:31 UTC |
| start   | --driver=docker                                                      | minikube | goutham | v1.36.0 | 17 Jul 25 12:39 UTC | 17 Jul 25 12:40 UTC |
| service | cloudblade-service                                                   | minikube | goutham | v1.36.0 | 17 Jul 25 12:54 UTC | 17 Jul 25 12:56 UTC |
| start   |                                                                      | minikube | goutham | v1.36.0 | 21 Jul 25 13:17 UTC | 21 Jul 25 13:19 UTC |
| ip      |                                                                      | minikube | goutham | v1.36.0 | 21 Jul 25 13:29 UTC | 21 Jul 25 13:29 UTC |
| ssh     |                                                                      | minikube | goutham | v1.36.0 | 21 Jul 25 13:40 UTC |                     |
| stop    |                                                                      | minikube | goutham | v1.36.0 | 21 Jul 25 14:03 UTC |                     |
| delete  |                                                                      | minikube | goutham | v1.36.0 | 21 Jul 25 14:04 UTC | 21 Jul 25 14:04 UTC |
| start   |                                                                      | minikube | goutham | v1.36.0 | 23 Jul 25 12:34 UTC |                     |
| stop    |                                                                      | minikube | goutham | v1.36.0 | 23 Jul 25 12:36 UTC | 23 Jul 25 12:36 UTC |
| delete  |                                                                      | minikube | goutham | v1.36.0 | 23 Jul 25 12:36 UTC | 23 Jul 25 12:36 UTC |
| start   | --driver=docker --memory=4096                                        | minikube | goutham | v1.36.0 | 23 Jul 25 12:36 UTC |                     |
|         | --cpus=2                                                             |          |         |         |                     |                     |
| start   | --driver=docker --memory=3500                                        | minikube | goutham | v1.36.0 | 23 Jul 25 12:36 UTC | 23 Jul 25 12:37 UTC |
|         | --cpus=2                                                             |          |         |         |                     |                     |
|---------|----------------------------------------------------------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/07/23 12:36:52
Running on machine: Gouthams-Laptop
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0723 12:36:52.344778   17168 out.go:345] Setting OutFile to fd 1 ...
I0723 12:36:52.347144   17168 out.go:397] isatty.IsTerminal(1) = true
I0723 12:36:52.347149   17168 out.go:358] Setting ErrFile to fd 2...
I0723 12:36:52.347157   17168 out.go:397] isatty.IsTerminal(2) = true
I0723 12:36:52.347831   17168 root.go:338] Updating PATH: /home/goutham/.minikube/bin
I0723 12:36:52.348620   17168 out.go:352] Setting JSON to false
I0723 12:36:52.350182   17168 start.go:130] hostinfo: {"hostname":"Gouthams-Laptop","uptime":7631,"bootTime":1753266582,"procs":46,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.6.87.2-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"guest","hostId":"05139adb-280b-4474-a049-5befa57e3677"}
I0723 12:36:52.351995   17168 start.go:140] virtualization: kvm guest
I0723 12:36:52.358014   17168 out.go:177] 😄  minikube v1.36.0 on Ubuntu 24.04 (kvm/amd64)
I0723 12:36:52.368639   17168 notify.go:220] Checking for updates...
I0723 12:36:52.369737   17168 driver.go:404] Setting default libvirt URI to qemu:///system
I0723 12:36:52.578643   17168 docker.go:123] docker version: linux-28.3.2:Docker Engine - Community
I0723 12:36:52.578711   17168 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0723 12:36:56.358829   17168 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.780096155s)
I0723 12:36:56.359228   17168 info.go:266] docker info: {ID:78884195-ae00-4e4c-ba60-30c063a82675 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:34 OomKillDisable:false NGoroutines:52 SystemTime:2025-07-23 12:36:56.345364588 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4020469760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Gouthams-Laptop Labels:[] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:/usr/local/lib/docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/usr/local/lib/docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:/usr/local/lib/docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I0723 12:36:56.359321   17168 docker.go:318] overlay module found
I0723 12:36:56.364933   17168 out.go:177] ✨  Using the docker driver based on user configuration
I0723 12:36:56.370351   17168 start.go:304] selected driver: docker
I0723 12:36:56.370366   17168 start.go:908] validating driver "docker" against <nil>
I0723 12:36:56.370375   17168 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0723 12:36:56.376811   17168 out.go:201] 
W0723 12:36:56.383774   17168 out.go:270] 🧯  The requested memory allocation of 3500MiB does not leave room for system overhead (total system memory: 3834MiB). You may face stability issues.
W0723 12:36:56.383889   17168 out.go:270] 💡  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=3500mb'
I0723 12:36:56.389511   17168 out.go:201] 
I0723 12:36:56.395314   17168 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0723 12:36:56.602408   17168 info.go:266] docker info: {ID:78884195-ae00-4e4c-ba60-30c063a82675 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:19 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:34 OomKillDisable:false NGoroutines:52 SystemTime:2025-07-23 12:36:56.591333178 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4020469760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Gouthams-Laptop Labels:[] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:/usr/local/lib/docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/usr/local/lib/docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:/usr/local/lib/docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I0723 12:36:56.602558   17168 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0723 12:36:56.603352   17168 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0723 12:36:56.608487   17168 out.go:177] 📌  Using Docker driver with root privileges
I0723 12:36:56.613550   17168 cni.go:84] Creating CNI manager for ""
I0723 12:36:56.613688   17168 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0723 12:36:56.613711   17168 start_flags.go:320] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0723 12:36:56.613899   17168 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/goutham:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0723 12:36:56.619605   17168 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0723 12:36:56.624893   17168 cache.go:121] Beginning downloading kic base image for docker with docker
I0723 12:36:56.629988   17168 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0723 12:36:56.635198   17168 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0723 12:36:56.635246   17168 preload.go:146] Found local preload: /home/goutham/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0723 12:36:56.635251   17168 cache.go:56] Caching tarball of preloaded images
I0723 12:36:56.635330   17168 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0723 12:36:56.635384   17168 preload.go:172] Found /home/goutham/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0723 12:36:56.635391   17168 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0723 12:36:56.635679   17168 profile.go:143] Saving config to /home/goutham/.minikube/profiles/minikube/config.json ...
I0723 12:36:56.635696   17168 lock.go:35] WriteFile acquiring /home/goutham/.minikube/profiles/minikube/config.json: {Name:mk2a9cbccac6c53bd7f1bd3834aa20a32686730e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:36:56.670965   17168 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0723 12:36:56.670972   17168 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0723 12:36:56.670999   17168 cache.go:230] Successfully downloaded all kic artifacts
I0723 12:36:56.671046   17168 start.go:360] acquireMachinesLock for minikube: {Name:mk899d0452dc8c6582dd3b3be110ed5f4cb51ffa Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0723 12:36:56.671103   17168 start.go:364] duration metric: took 47.959µs to acquireMachinesLock for "minikube"
I0723 12:36:56.671116   17168 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/goutham:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0723 12:36:56.671160   17168 start.go:125] createHost starting for "" (driver="docker")
I0723 12:36:56.676736   17168 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=3500MB) ...
I0723 12:36:56.677812   17168 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0723 12:36:56.677857   17168 client.go:168] LocalClient.Create starting
I0723 12:36:56.677929   17168 main.go:141] libmachine: Reading certificate data from /home/goutham/.minikube/certs/ca.pem
I0723 12:36:56.678250   17168 main.go:141] libmachine: Decoding PEM data...
I0723 12:36:56.678270   17168 main.go:141] libmachine: Parsing certificate...
I0723 12:36:56.678797   17168 main.go:141] libmachine: Reading certificate data from /home/goutham/.minikube/certs/cert.pem
I0723 12:36:56.679235   17168 main.go:141] libmachine: Decoding PEM data...
I0723 12:36:56.679247   17168 main.go:141] libmachine: Parsing certificate...
I0723 12:36:56.680456   17168 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0723 12:36:56.699228   17168 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0723 12:36:56.699289   17168 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0723 12:36:56.699306   17168 cli_runner.go:164] Run: docker network inspect minikube
W0723 12:36:56.716900   17168 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0723 12:36:56.716913   17168 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0723 12:36:56.716920   17168 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0723 12:36:56.717041   17168 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0723 12:36:56.734688   17168 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001a7a640}
I0723 12:36:56.734721   17168 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0723 12:36:56.734785   17168 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0723 12:36:56.832107   17168 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0723 12:36:56.832122   17168 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0723 12:36:56.832171   17168 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0723 12:36:56.849881   17168 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0723 12:36:56.875257   17168 oci.go:103] Successfully created a docker volume minikube
I0723 12:36:56.875320   17168 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0723 12:36:58.045087   17168 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: (1.169742302s)
I0723 12:36:58.045101   17168 oci.go:107] Successfully prepared a docker volume minikube
I0723 12:36:58.045143   17168 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0723 12:36:58.045157   17168 kic.go:194] Starting extracting preloaded images to volume ...
I0723 12:36:58.045206   17168 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/goutham/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0723 12:37:08.018987   17168 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/goutham/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (9.930435717s)
I0723 12:37:08.019126   17168 kic.go:203] duration metric: took 9.930664831s to extract preloaded images to volume ...
W0723 12:37:08.019420   17168 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0723 12:37:08.019510   17168 oci.go:249] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0723 12:37:08.019623   17168 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0723 12:37:08.272534   17168 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3500mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0723 12:37:08.836052   17168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0723 12:37:08.856688   17168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0723 12:37:08.877099   17168 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0723 12:37:08.960871   17168 oci.go:144] the created container "minikube" has a running status.
I0723 12:37:08.962050   17168 kic.go:225] Creating ssh key for kic: /home/goutham/.minikube/machines/minikube/id_rsa...
I0723 12:37:09.242529   17168 kic_runner.go:191] docker (temp): /home/goutham/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0723 12:37:09.289819   17168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0723 12:37:09.319282   17168 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0723 12:37:09.319293   17168 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0723 12:37:09.393611   17168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0723 12:37:09.423602   17168 machine.go:93] provisionDockerMachine start ...
I0723 12:37:09.423898   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:09.462943   17168 main.go:141] libmachine: Using SSH client type: native
I0723 12:37:09.463683   17168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0723 12:37:09.463692   17168 main.go:141] libmachine: About to run SSH command:
hostname
I0723 12:37:09.617750   17168 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0723 12:37:09.617765   17168 ubuntu.go:169] provisioning hostname "minikube"
I0723 12:37:09.617814   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:09.646182   17168 main.go:141] libmachine: Using SSH client type: native
I0723 12:37:09.646498   17168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0723 12:37:09.646505   17168 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0723 12:37:09.840963   17168 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0723 12:37:09.841036   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:09.867172   17168 main.go:141] libmachine: Using SSH client type: native
I0723 12:37:09.867753   17168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0723 12:37:09.867781   17168 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0723 12:37:10.007204   17168 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0723 12:37:10.007534   17168 ubuntu.go:175] set auth options {CertDir:/home/goutham/.minikube CaCertPath:/home/goutham/.minikube/certs/ca.pem CaPrivateKeyPath:/home/goutham/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/goutham/.minikube/machines/server.pem ServerKeyPath:/home/goutham/.minikube/machines/server-key.pem ClientKeyPath:/home/goutham/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/goutham/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/goutham/.minikube}
I0723 12:37:10.007550   17168 ubuntu.go:177] setting up certificates
I0723 12:37:10.007557   17168 provision.go:84] configureAuth start
I0723 12:37:10.007642   17168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0723 12:37:10.038682   17168 provision.go:143] copyHostCerts
I0723 12:37:10.039207   17168 exec_runner.go:144] found /home/goutham/.minikube/ca.pem, removing ...
I0723 12:37:10.039307   17168 exec_runner.go:203] rm: /home/goutham/.minikube/ca.pem
I0723 12:37:10.042541   17168 exec_runner.go:151] cp: /home/goutham/.minikube/certs/ca.pem --> /home/goutham/.minikube/ca.pem (1082 bytes)
I0723 12:37:10.042749   17168 exec_runner.go:144] found /home/goutham/.minikube/cert.pem, removing ...
I0723 12:37:10.042755   17168 exec_runner.go:203] rm: /home/goutham/.minikube/cert.pem
I0723 12:37:10.043266   17168 exec_runner.go:151] cp: /home/goutham/.minikube/certs/cert.pem --> /home/goutham/.minikube/cert.pem (1123 bytes)
I0723 12:37:10.043422   17168 exec_runner.go:144] found /home/goutham/.minikube/key.pem, removing ...
I0723 12:37:10.043428   17168 exec_runner.go:203] rm: /home/goutham/.minikube/key.pem
I0723 12:37:10.043979   17168 exec_runner.go:151] cp: /home/goutham/.minikube/certs/key.pem --> /home/goutham/.minikube/key.pem (1675 bytes)
I0723 12:37:10.044430   17168 provision.go:117] generating server cert: /home/goutham/.minikube/machines/server.pem ca-key=/home/goutham/.minikube/certs/ca.pem private-key=/home/goutham/.minikube/certs/ca-key.pem org=goutham.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0723 12:37:10.306220   17168 provision.go:177] copyRemoteCerts
I0723 12:37:10.306262   17168 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0723 12:37:10.306292   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:10.329563   17168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/goutham/.minikube/machines/minikube/id_rsa Username:docker}
I0723 12:37:10.424715   17168 ssh_runner.go:362] scp /home/goutham/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0723 12:37:10.459471   17168 ssh_runner.go:362] scp /home/goutham/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0723 12:37:10.488913   17168 ssh_runner.go:362] scp /home/goutham/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0723 12:37:10.520621   17168 provision.go:87] duration metric: took 513.053646ms to configureAuth
I0723 12:37:10.520634   17168 ubuntu.go:193] setting minikube options for container-runtime
I0723 12:37:10.520910   17168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0723 12:37:10.520997   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:10.545718   17168 main.go:141] libmachine: Using SSH client type: native
I0723 12:37:10.545992   17168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0723 12:37:10.546003   17168 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0723 12:37:10.680077   17168 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0723 12:37:10.680091   17168 ubuntu.go:71] root file system type: overlay
I0723 12:37:10.680454   17168 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0723 12:37:10.680532   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:10.709998   17168 main.go:141] libmachine: Using SSH client type: native
I0723 12:37:10.710217   17168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0723 12:37:10.710268   17168 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0723 12:37:10.861810   17168 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0723 12:37:10.861861   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:10.886434   17168 main.go:141] libmachine: Using SSH client type: native
I0723 12:37:10.886628   17168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0723 12:37:10.886638   17168 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0723 12:37:12.918447   17168 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-07-23 12:37:10.853971444 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0723 12:37:12.918467   17168 machine.go:96] duration metric: took 3.49484997s to provisionDockerMachine
I0723 12:37:12.918474   17168 client.go:171] duration metric: took 16.197339774s to LocalClient.Create
I0723 12:37:12.918531   17168 start.go:167] duration metric: took 16.197423983s to libmachine.API.Create "minikube"
I0723 12:37:12.918538   17168 start.go:293] postStartSetup for "minikube" (driver="docker")
I0723 12:37:12.918550   17168 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0723 12:37:12.918600   17168 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0723 12:37:12.918632   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:12.939473   17168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/goutham/.minikube/machines/minikube/id_rsa Username:docker}
I0723 12:37:13.034513   17168 ssh_runner.go:195] Run: cat /etc/os-release
I0723 12:37:13.038628   17168 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0723 12:37:13.038663   17168 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0723 12:37:13.038677   17168 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0723 12:37:13.038683   17168 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0723 12:37:13.038690   17168 filesync.go:126] Scanning /home/goutham/.minikube/addons for local assets ...
I0723 12:37:13.039145   17168 filesync.go:126] Scanning /home/goutham/.minikube/files for local assets ...
I0723 12:37:13.039444   17168 start.go:296] duration metric: took 120.899204ms for postStartSetup
I0723 12:37:13.039956   17168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0723 12:37:13.058430   17168 profile.go:143] Saving config to /home/goutham/.minikube/profiles/minikube/config.json ...
I0723 12:37:13.058674   17168 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0723 12:37:13.058711   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:13.077192   17168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/goutham/.minikube/machines/minikube/id_rsa Username:docker}
I0723 12:37:13.169814   17168 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0723 12:37:13.175003   17168 start.go:128] duration metric: took 16.46052632s to createHost
I0723 12:37:13.175015   17168 start.go:83] releasing machines lock for "minikube", held for 16.46063257s
I0723 12:37:13.175071   17168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0723 12:37:13.193101   17168 ssh_runner.go:195] Run: cat /version.json
I0723 12:37:13.193150   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:13.193259   17168 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0723 12:37:13.193297   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:13.213999   17168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/goutham/.minikube/machines/minikube/id_rsa Username:docker}
I0723 12:37:13.215394   17168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/goutham/.minikube/machines/minikube/id_rsa Username:docker}
I0723 12:37:13.307626   17168 ssh_runner.go:195] Run: systemctl --version
I0723 12:37:13.519481   17168 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0723 12:37:13.526445   17168 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0723 12:37:13.560800   17168 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0723 12:37:13.560878   17168 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0723 12:37:13.603800   17168 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0723 12:37:13.603812   17168 start.go:495] detecting cgroup driver to use...
I0723 12:37:13.603834   17168 detect.go:190] detected "systemd" cgroup driver on host os
I0723 12:37:13.604526   17168 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0723 12:37:13.624416   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0723 12:37:13.637220   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0723 12:37:13.649203   17168 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0723 12:37:13.649258   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0723 12:37:13.662155   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0723 12:37:13.674770   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0723 12:37:13.686763   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0723 12:37:13.698538   17168 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0723 12:37:13.709698   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0723 12:37:13.721700   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0723 12:37:13.733400   17168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0723 12:37:13.745464   17168 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0723 12:37:13.756733   17168 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0723 12:37:13.767784   17168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0723 12:37:13.847349   17168 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0723 12:37:13.967434   17168 start.go:495] detecting cgroup driver to use...
I0723 12:37:13.967466   17168 detect.go:190] detected "systemd" cgroup driver on host os
I0723 12:37:13.967511   17168 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0723 12:37:13.996350   17168 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0723 12:37:13.996428   17168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0723 12:37:14.014427   17168 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0723 12:37:14.039784   17168 ssh_runner.go:195] Run: which cri-dockerd
I0723 12:37:14.044622   17168 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0723 12:37:14.057486   17168 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0723 12:37:14.080469   17168 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0723 12:37:14.156338   17168 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0723 12:37:14.239984   17168 docker.go:587] configuring docker to use "systemd" as cgroup driver...
I0723 12:37:14.240172   17168 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0723 12:37:14.261328   17168 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0723 12:37:14.276116   17168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0723 12:37:14.354736   17168 ssh_runner.go:195] Run: sudo systemctl restart docker
I0723 12:37:17.075482   17168 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.720723969s)
I0723 12:37:17.075541   17168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0723 12:37:17.096881   17168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0723 12:37:17.110546   17168 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0723 12:37:17.194909   17168 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0723 12:37:17.275624   17168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0723 12:37:17.345593   17168 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0723 12:37:17.389493   17168 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0723 12:37:17.402438   17168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0723 12:37:17.488177   17168 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0723 12:37:17.895453   17168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0723 12:37:17.910634   17168 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0723 12:37:17.910678   17168 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0723 12:37:17.915397   17168 start.go:563] Will wait 60s for crictl version
I0723 12:37:17.915434   17168 ssh_runner.go:195] Run: which crictl
I0723 12:37:17.920586   17168 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0723 12:37:18.122897   17168 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0723 12:37:18.122950   17168 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0723 12:37:18.313676   17168 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0723 12:37:18.355006   17168 out.go:235] 🐳  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0723 12:37:18.355195   17168 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0723 12:37:18.384067   17168 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0723 12:37:18.390456   17168 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0723 12:37:18.412215   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0723 12:37:18.432708   17168 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/goutham:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0723 12:37:18.432810   17168 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0723 12:37:18.432853   17168 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0723 12:37:18.455983   17168 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0723 12:37:18.455994   17168 docker.go:632] Images already preloaded, skipping extraction
I0723 12:37:18.456058   17168 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0723 12:37:18.478047   17168 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0723 12:37:18.478057   17168 cache_images.go:84] Images are preloaded, skipping loading
I0723 12:37:18.478064   17168 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0723 12:37:18.478526   17168 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0723 12:37:18.478572   17168 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0723 12:37:18.862512   17168 cni.go:84] Creating CNI manager for ""
I0723 12:37:18.862524   17168 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0723 12:37:18.862532   17168 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0723 12:37:18.862545   17168 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0723 12:37:18.862659   17168 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0723 12:37:18.862730   17168 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0723 12:37:18.875652   17168 binaries.go:44] Found k8s binaries, skipping transfer
I0723 12:37:18.875703   17168 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0723 12:37:18.886864   17168 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0723 12:37:18.907865   17168 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0723 12:37:18.929050   17168 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0723 12:37:18.949545   17168 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0723 12:37:18.953824   17168 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0723 12:37:18.968083   17168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0723 12:37:19.041792   17168 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0723 12:37:19.089224   17168 certs.go:68] Setting up /home/goutham/.minikube/profiles/minikube for IP: 192.168.49.2
I0723 12:37:19.089250   17168 certs.go:194] generating shared ca certs ...
I0723 12:37:19.089264   17168 certs.go:226] acquiring lock for ca certs: {Name:mkdb5e9facba998024a448a5af3bd36ca1c707d5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:19.089716   17168 certs.go:235] skipping valid "minikubeCA" ca cert: /home/goutham/.minikube/ca.key
I0723 12:37:19.089988   17168 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/goutham/.minikube/proxy-client-ca.key
I0723 12:37:19.089997   17168 certs.go:256] generating profile certs ...
I0723 12:37:19.090076   17168 certs.go:363] generating signed profile cert for "minikube-user": /home/goutham/.minikube/profiles/minikube/client.key
I0723 12:37:19.090088   17168 crypto.go:68] Generating cert /home/goutham/.minikube/profiles/minikube/client.crt with IP's: []
I0723 12:37:19.205275   17168 crypto.go:156] Writing cert to /home/goutham/.minikube/profiles/minikube/client.crt ...
I0723 12:37:19.205286   17168 lock.go:35] WriteFile acquiring /home/goutham/.minikube/profiles/minikube/client.crt: {Name:mk3467d2f0d95668ba65555cda659242738c2f42 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:19.205464   17168 crypto.go:164] Writing key to /home/goutham/.minikube/profiles/minikube/client.key ...
I0723 12:37:19.205469   17168 lock.go:35] WriteFile acquiring /home/goutham/.minikube/profiles/minikube/client.key: {Name:mk89d2f9aa0d11ba4d134cd4657c01c3b4db2d7b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:19.205564   17168 certs.go:363] generating signed profile cert for "minikube": /home/goutham/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0723 12:37:19.205643   17168 crypto.go:68] Generating cert /home/goutham/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0723 12:37:19.324293   17168 crypto.go:156] Writing cert to /home/goutham/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0723 12:37:19.324305   17168 lock.go:35] WriteFile acquiring /home/goutham/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk3967e08cfc380a4c8422a179341faef0727d79 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:19.324449   17168 crypto.go:164] Writing key to /home/goutham/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0723 12:37:19.324454   17168 lock.go:35] WriteFile acquiring /home/goutham/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkacac9dd0c6c3757931e814fb557ba39809c28e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:19.324539   17168 certs.go:381] copying /home/goutham/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/goutham/.minikube/profiles/minikube/apiserver.crt
I0723 12:37:19.324635   17168 certs.go:385] copying /home/goutham/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/goutham/.minikube/profiles/minikube/apiserver.key
I0723 12:37:19.324742   17168 certs.go:363] generating signed profile cert for "aggregator": /home/goutham/.minikube/profiles/minikube/proxy-client.key
I0723 12:37:19.324754   17168 crypto.go:68] Generating cert /home/goutham/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0723 12:37:19.387260   17168 crypto.go:156] Writing cert to /home/goutham/.minikube/profiles/minikube/proxy-client.crt ...
I0723 12:37:19.387273   17168 lock.go:35] WriteFile acquiring /home/goutham/.minikube/profiles/minikube/proxy-client.crt: {Name:mk3207c2b20bc26f6313cb6b54859ae20ca35f7f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:19.387429   17168 crypto.go:164] Writing key to /home/goutham/.minikube/profiles/minikube/proxy-client.key ...
I0723 12:37:19.387436   17168 lock.go:35] WriteFile acquiring /home/goutham/.minikube/profiles/minikube/proxy-client.key: {Name:mk321b4ade675acccf61f7f31aa413ee8d88d7cb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:19.388163   17168 certs.go:484] found cert: /home/goutham/.minikube/certs/ca-key.pem (1675 bytes)
I0723 12:37:19.388210   17168 certs.go:484] found cert: /home/goutham/.minikube/certs/ca.pem (1082 bytes)
I0723 12:37:19.388241   17168 certs.go:484] found cert: /home/goutham/.minikube/certs/cert.pem (1123 bytes)
I0723 12:37:19.388276   17168 certs.go:484] found cert: /home/goutham/.minikube/certs/key.pem (1675 bytes)
I0723 12:37:19.389117   17168 ssh_runner.go:362] scp /home/goutham/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0723 12:37:19.422635   17168 ssh_runner.go:362] scp /home/goutham/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0723 12:37:19.450455   17168 ssh_runner.go:362] scp /home/goutham/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0723 12:37:19.478238   17168 ssh_runner.go:362] scp /home/goutham/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0723 12:37:19.507238   17168 ssh_runner.go:362] scp /home/goutham/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0723 12:37:19.536044   17168 ssh_runner.go:362] scp /home/goutham/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0723 12:37:19.563519   17168 ssh_runner.go:362] scp /home/goutham/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0723 12:37:19.591716   17168 ssh_runner.go:362] scp /home/goutham/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0723 12:37:19.620388   17168 ssh_runner.go:362] scp /home/goutham/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0723 12:37:19.648991   17168 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0723 12:37:19.669830   17168 ssh_runner.go:195] Run: openssl version
I0723 12:37:19.686933   17168 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0723 12:37:19.700856   17168 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0723 12:37:19.705416   17168 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul 16 11:31 /usr/share/ca-certificates/minikubeCA.pem
I0723 12:37:19.705454   17168 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0723 12:37:19.713629   17168 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0723 12:37:19.724907   17168 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0723 12:37:19.729056   17168 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0723 12:37:19.729083   17168 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/goutham:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0723 12:37:19.729153   17168 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0723 12:37:19.755377   17168 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0723 12:37:19.766494   17168 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0723 12:37:19.777186   17168 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0723 12:37:19.777225   17168 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0723 12:37:19.788522   17168 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0723 12:37:19.788532   17168 kubeadm.go:157] found existing configuration files:

I0723 12:37:19.788569   17168 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0723 12:37:19.799534   17168 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0723 12:37:19.799575   17168 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0723 12:37:19.810305   17168 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0723 12:37:19.820872   17168 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0723 12:37:19.820909   17168 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0723 12:37:19.831105   17168 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0723 12:37:19.841494   17168 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0723 12:37:19.841545   17168 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0723 12:37:19.852287   17168 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0723 12:37:19.862735   17168 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0723 12:37:19.862772   17168 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0723 12:37:19.872856   17168 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0723 12:37:20.152945   17168 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0723 12:37:20.152995   17168 kubeadm.go:310] [preflight] Running pre-flight checks
I0723 12:37:20.298565   17168 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0723 12:37:20.298691   17168 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0723 12:37:20.298818   17168 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0723 12:37:20.316975   17168 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0723 12:37:20.329061   17168 out.go:235]     ▪ Generating certificates and keys ...
I0723 12:37:20.329263   17168 kubeadm.go:310] [certs] Using existing ca certificate authority
I0723 12:37:20.329329   17168 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0723 12:37:20.359621   17168 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0723 12:37:20.564007   17168 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0723 12:37:20.833175   17168 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0723 12:37:21.077988   17168 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0723 12:37:21.394931   17168 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0723 12:37:21.395011   17168 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0723 12:37:21.567636   17168 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0723 12:37:21.567726   17168 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0723 12:37:22.237203   17168 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0723 12:37:22.521661   17168 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0723 12:37:22.632183   17168 kubeadm.go:310] [certs] Generating "sa" key and public key
I0723 12:37:22.632230   17168 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0723 12:37:22.713615   17168 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0723 12:37:22.875476   17168 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0723 12:37:23.060471   17168 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0723 12:37:23.286101   17168 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0723 12:37:23.499948   17168 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0723 12:37:23.500845   17168 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0723 12:37:23.504392   17168 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0723 12:37:23.510108   17168 out.go:235]     ▪ Booting up control plane ...
I0723 12:37:23.510382   17168 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0723 12:37:23.510444   17168 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0723 12:37:23.510506   17168 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0723 12:37:23.518478   17168 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0723 12:37:23.526260   17168 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0723 12:37:23.526333   17168 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0723 12:37:23.660977   17168 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0723 12:37:23.661151   17168 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0723 12:37:24.162202   17168 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.431944ms
I0723 12:37:24.164302   17168 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0723 12:37:24.164398   17168 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0723 12:37:24.164482   17168 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0723 12:37:24.164591   17168 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0723 12:37:26.330038   17168 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 2.165430018s
I0723 12:37:28.102405   17168 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 3.93816967s
I0723 12:37:30.168286   17168 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 6.003627937s
I0723 12:37:30.193437   17168 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0723 12:37:30.218715   17168 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0723 12:37:30.265036   17168 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0723 12:37:30.265352   17168 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0723 12:37:30.278030   17168 kubeadm.go:310] [bootstrap-token] Using token: 9i49y7.rgq0cwsjdrhk90mp
I0723 12:37:30.283462   17168 out.go:235]     ▪ Configuring RBAC rules ...
I0723 12:37:30.283704   17168 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0723 12:37:30.290266   17168 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0723 12:37:30.302256   17168 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0723 12:37:30.310653   17168 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0723 12:37:30.316211   17168 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0723 12:37:30.321836   17168 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0723 12:37:30.577475   17168 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0723 12:37:31.027676   17168 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0723 12:37:31.576278   17168 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0723 12:37:31.576858   17168 kubeadm.go:310] 
I0723 12:37:31.576923   17168 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0723 12:37:31.576926   17168 kubeadm.go:310] 
I0723 12:37:31.576978   17168 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0723 12:37:31.576980   17168 kubeadm.go:310] 
I0723 12:37:31.577006   17168 kubeadm.go:310]   mkdir -p $HOME/.kube
I0723 12:37:31.577073   17168 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0723 12:37:31.577106   17168 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0723 12:37:31.577108   17168 kubeadm.go:310] 
I0723 12:37:31.577141   17168 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0723 12:37:31.577142   17168 kubeadm.go:310] 
I0723 12:37:31.577205   17168 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0723 12:37:31.577208   17168 kubeadm.go:310] 
I0723 12:37:31.577240   17168 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0723 12:37:31.577338   17168 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0723 12:37:31.577437   17168 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0723 12:37:31.577441   17168 kubeadm.go:310] 
I0723 12:37:31.577529   17168 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0723 12:37:31.577583   17168 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0723 12:37:31.577593   17168 kubeadm.go:310] 
I0723 12:37:31.577649   17168 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 9i49y7.rgq0cwsjdrhk90mp \
I0723 12:37:31.577725   17168 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a05f12b5b22555a7a6c8c3a7004ea79015b44ea9c59ad3bbcefab98e0873a9da \
I0723 12:37:31.577738   17168 kubeadm.go:310] 	--control-plane 
I0723 12:37:31.577740   17168 kubeadm.go:310] 
I0723 12:37:31.577835   17168 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0723 12:37:31.577838   17168 kubeadm.go:310] 
I0723 12:37:31.577892   17168 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 9i49y7.rgq0cwsjdrhk90mp \
I0723 12:37:31.578019   17168 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a05f12b5b22555a7a6c8c3a7004ea79015b44ea9c59ad3bbcefab98e0873a9da 
I0723 12:37:31.582142   17168 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0723 12:37:31.582258   17168 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0723 12:37:31.582274   17168 cni.go:84] Creating CNI manager for ""
I0723 12:37:31.582283   17168 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0723 12:37:31.587856   17168 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0723 12:37:31.593703   17168 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0723 12:37:31.605038   17168 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0723 12:37:31.627278   17168 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0723 12:37:31.627533   17168 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0723 12:37:31.627577   17168 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_07_23T12_37_31_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0723 12:37:31.638912   17168 ops.go:34] apiserver oom_adj: -16
I0723 12:37:31.903588   17168 kubeadm.go:1105] duration metric: took 276.118695ms to wait for elevateKubeSystemPrivileges
I0723 12:37:31.914131   17168 kubeadm.go:394] duration metric: took 12.185044095s to StartCluster
I0723 12:37:31.914152   17168 settings.go:142] acquiring lock: {Name:mka878323c664bfe880ea39bfb822dcbcfcab0b2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:31.914248   17168 settings.go:150] Updating kubeconfig:  /home/goutham/.kube/config
I0723 12:37:31.916043   17168 lock.go:35] WriteFile acquiring /home/goutham/.kube/config: {Name:mk783fec42f3bd87ecf9499e159b37de468ca382 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0723 12:37:31.916262   17168 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0723 12:37:31.916401   17168 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0723 12:37:31.916601   17168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0723 12:37:31.916708   17168 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0723 12:37:31.916791   17168 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0723 12:37:31.916807   17168 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0723 12:37:31.916840   17168 host.go:66] Checking if "minikube" exists ...
I0723 12:37:31.916842   17168 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0723 12:37:31.916858   17168 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0723 12:37:31.917276   17168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0723 12:37:31.917400   17168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0723 12:37:31.922032   17168 out.go:177] 🔎  Verifying Kubernetes components...
I0723 12:37:31.934462   17168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0723 12:37:31.950411   17168 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0723 12:37:31.960046   17168 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0723 12:37:31.960058   17168 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0723 12:37:31.960143   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:31.980998   17168 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0723 12:37:31.981042   17168 host.go:66] Checking if "minikube" exists ...
I0723 12:37:31.982327   17168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0723 12:37:32.013757   17168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/goutham/.minikube/machines/minikube/id_rsa Username:docker}
I0723 12:37:32.036218   17168 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0723 12:37:32.036231   17168 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0723 12:37:32.036311   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0723 12:37:32.071861   17168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/goutham/.minikube/machines/minikube/id_rsa Username:docker}
I0723 12:37:32.097294   17168 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0723 12:37:32.166213   17168 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0723 12:37:32.174578   17168 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0723 12:37:32.216134   17168 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0723 12:37:32.440800   17168 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0723 12:37:32.441135   17168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0723 12:37:32.470150   17168 api_server.go:52] waiting for apiserver process to appear ...
I0723 12:37:32.470241   17168 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0723 12:37:32.618386   17168 api_server.go:72] duration metric: took 658.247142ms to wait for apiserver process to appear ...
I0723 12:37:32.618408   17168 api_server.go:88] waiting for apiserver healthz status ...
I0723 12:37:32.618424   17168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32776/healthz ...
I0723 12:37:32.624324   17168 api_server.go:279] https://127.0.0.1:32776/healthz returned 200:
ok
I0723 12:37:32.625579   17168 api_server.go:141] control plane version: v1.33.1
I0723 12:37:32.625591   17168 api_server.go:131] duration metric: took 7.177565ms to wait for apiserver health ...
I0723 12:37:32.626004   17168 system_pods.go:43] waiting for kube-system pods to appear ...
I0723 12:37:32.630214   17168 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0723 12:37:32.636467   17168 addons.go:514] duration metric: took 676.190799ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0723 12:37:32.636840   17168 system_pods.go:59] 5 kube-system pods found
I0723 12:37:32.636857   17168 system_pods.go:61] "etcd-minikube" [de61b0c6-7539-4f30-98b6-8f4efb7f08dd] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0723 12:37:32.636863   17168 system_pods.go:61] "kube-apiserver-minikube" [295408a8-a3d9-4f03-bd65-ef2a4f0db0e9] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0723 12:37:32.636868   17168 system_pods.go:61] "kube-controller-manager-minikube" [501516d8-4b48-47f9-af68-d95454694d34] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0723 12:37:32.636874   17168 system_pods.go:61] "kube-scheduler-minikube" [4b9570be-914e-422b-a96c-d367e077b313] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0723 12:37:32.636876   17168 system_pods.go:61] "storage-provisioner" [0acc64b0-8ae2-4286-a207-31c5bcb0dd79] Pending
I0723 12:37:32.636881   17168 system_pods.go:74] duration metric: took 10.871241ms to wait for pod list to return data ...
I0723 12:37:32.636889   17168 kubeadm.go:578] duration metric: took 676.758776ms to wait for: map[apiserver:true system_pods:true]
I0723 12:37:32.636898   17168 node_conditions.go:102] verifying NodePressure condition ...
I0723 12:37:32.640547   17168 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0723 12:37:32.640722   17168 node_conditions.go:123] node cpu capacity is 8
I0723 12:37:32.641163   17168 node_conditions.go:105] duration metric: took 4.258516ms to run NodePressure ...
I0723 12:37:32.641174   17168 start.go:241] waiting for startup goroutines ...
I0723 12:37:32.947358   17168 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0723 12:37:32.947373   17168 start.go:246] waiting for cluster config update ...
I0723 12:37:32.947380   17168 start.go:255] writing updated cluster config ...
I0723 12:37:32.947625   17168 ssh_runner.go:195] Run: rm -f paused
I0723 12:37:33.418599   17168 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0723 12:37:33.425040   17168 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 23 12:37:13 minikube dockerd[679]: time="2025-07-23T12:37:13.860911722Z" level=info msg="Daemon shutdown complete"
Jul 23 12:37:13 minikube dockerd[679]: time="2025-07-23T12:37:13.861013580Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Jul 23 12:37:13 minikube systemd[1]: docker.service: Deactivated successfully.
Jul 23 12:37:13 minikube systemd[1]: Stopped Docker Application Container Engine.
Jul 23 12:37:13 minikube systemd[1]: Starting Docker Application Container Engine...
Jul 23 12:37:14 minikube dockerd[1029]: time="2025-07-23T12:37:14.037168330Z" level=info msg="Starting up"
Jul 23 12:37:14 minikube dockerd[1029]: time="2025-07-23T12:37:14.039025129Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jul 23 12:37:14 minikube dockerd[1029]: time="2025-07-23T12:37:14.052939737Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jul 23 12:37:14 minikube dockerd[1029]: time="2025-07-23T12:37:14.094588646Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Jul 23 12:37:14 minikube dockerd[1029]: time="2025-07-23T12:37:14.099213774Z" level=info msg="Loading containers: start."
Jul 23 12:37:14 minikube dockerd[1029]: time="2025-07-23T12:37:14.366517059Z" level=info msg="Processing signal 'terminated'"
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.360794674Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 6d22dab4748a128973a2826c44d6f356ce51f7edb4b7b114d2ffaabdc20d5a71], retrying...."
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.429687594Z" level=info msg="Loading containers: done."
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.478723785Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.478797600Z" level=info msg="Initializing buildkit"
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.506034697Z" level=info msg="Completed buildkit initialization"
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.511821628Z" level=info msg="Daemon has completed initialization"
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.511927264Z" level=info msg="API listen on /var/run/docker.sock"
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.511958985Z" level=info msg="API listen on [::]:2376"
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.512959583Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jul 23 12:37:15 minikube dockerd[1029]: time="2025-07-23T12:37:15.513476191Z" level=info msg="Daemon shutdown complete"
Jul 23 12:37:15 minikube systemd[1]: docker.service: Deactivated successfully.
Jul 23 12:37:15 minikube systemd[1]: Stopped Docker Application Container Engine.
Jul 23 12:37:15 minikube systemd[1]: Starting Docker Application Container Engine...
Jul 23 12:37:15 minikube dockerd[1335]: time="2025-07-23T12:37:15.576145594Z" level=info msg="Starting up"
Jul 23 12:37:15 minikube dockerd[1335]: time="2025-07-23T12:37:15.577356965Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jul 23 12:37:15 minikube dockerd[1335]: time="2025-07-23T12:37:15.589707185Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jul 23 12:37:15 minikube dockerd[1335]: time="2025-07-23T12:37:15.606025284Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jul 23 12:37:15 minikube dockerd[1335]: time="2025-07-23T12:37:15.627846323Z" level=info msg="Loading containers: start."
Jul 23 12:37:16 minikube dockerd[1335]: time="2025-07-23T12:37:16.948789874Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 37dd5506a03000c7c24eddbc490aa427d689e3d06604606fbc651ba18ec62176], retrying...."
Jul 23 12:37:17 minikube dockerd[1335]: time="2025-07-23T12:37:17.025506709Z" level=info msg="Loading containers: done."
Jul 23 12:37:17 minikube dockerd[1335]: time="2025-07-23T12:37:17.043419257Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jul 23 12:37:17 minikube dockerd[1335]: time="2025-07-23T12:37:17.043503779Z" level=info msg="Initializing buildkit"
Jul 23 12:37:17 minikube dockerd[1335]: time="2025-07-23T12:37:17.069430396Z" level=info msg="Completed buildkit initialization"
Jul 23 12:37:17 minikube dockerd[1335]: time="2025-07-23T12:37:17.073075719Z" level=info msg="Daemon has completed initialization"
Jul 23 12:37:17 minikube dockerd[1335]: time="2025-07-23T12:37:17.073215958Z" level=info msg="API listen on [::]:2376"
Jul 23 12:37:17 minikube systemd[1]: Started Docker Application Container Engine.
Jul 23 12:37:17 minikube dockerd[1335]: time="2025-07-23T12:37:17.073258817Z" level=info msg="API listen on /var/run/docker.sock"
Jul 23 12:37:17 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Start docker client with request timeout 0s"
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Loaded network plugin cni"
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Docker cri networking managed by network plugin cni"
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Setting cgroupDriver systemd"
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jul 23 12:37:17 minikube cri-dockerd[1644]: time="2025-07-23T12:37:17Z" level=info msg="Start cri-dockerd grpc backend"
Jul 23 12:37:17 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jul 23 12:37:25 minikube cri-dockerd[1644]: time="2025-07-23T12:37:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6d0c140d8217053b86067583109694a99b54f566f6ebb4b0999beae77bd28933/resolv.conf as [nameserver 192.168.0.203 options ndots:0]"
Jul 23 12:37:25 minikube cri-dockerd[1644]: time="2025-07-23T12:37:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0de97b1e974782f5f0c3cc0a0da0ccd44eef1b9580f00bb92858cb6e950f5159/resolv.conf as [nameserver 192.168.0.203 options ndots:0]"
Jul 23 12:37:25 minikube cri-dockerd[1644]: time="2025-07-23T12:37:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0e23f9d0e987224551c5511f2572b3127aadc2758510e8d51e09be14e91f8f45/resolv.conf as [nameserver 192.168.0.203 options ndots:0]"
Jul 23 12:37:25 minikube cri-dockerd[1644]: time="2025-07-23T12:37:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d671ee408fdb9b9a79d02c1fa4e9e130a9c20dfa0902796591ec866f3d62680f/resolv.conf as [nameserver 192.168.0.203 options ndots:0]"
Jul 23 12:37:36 minikube cri-dockerd[1644]: time="2025-07-23T12:37:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4b5d4fb16b6f86bddd5a303e55967d9bc9193dd57aa929525b0c618e2050b82e/resolv.conf as [nameserver 192.168.0.203 options ndots:0]"
Jul 23 12:37:37 minikube cri-dockerd[1644]: time="2025-07-23T12:37:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/75150e7c9fb0e5a1e5a8d8657c27bb301f10787b620ef7d811d98e91565208d1/resolv.conf as [nameserver 192.168.0.203 options ndots:0]"
Jul 23 12:37:37 minikube cri-dockerd[1644]: time="2025-07-23T12:37:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6ca3d97120fda4563552d74b2b1c7304cb503fd338f96eff9c28e0b657243f4b/resolv.conf as [nameserver 192.168.0.203 options ndots:0]"
Jul 23 12:37:41 minikube cri-dockerd[1644]: time="2025-07-23T12:37:41Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jul 23 12:38:07 minikube dockerd[1335]: time="2025-07-23T12:38:07.628522695Z" level=info msg="ignoring event" container=b2ce48c5b6fb2a837bf36c7539a176fc364010fbad9b53523086b1dda5e2ea9d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 23 12:38:35 minikube cri-dockerd[1644]: time="2025-07-23T12:38:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7a06c82f81b8fb0c368f4ca293eea272c30d77f4580542238b9b8360648ba13e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
a71828e80f374       6e38f40d628db       34 seconds ago       Running             storage-provisioner       1                   6ca3d97120fda       storage-provisioner
b2ce48c5b6fb2       6e38f40d628db       About a minute ago   Exited              storage-provisioner       0                   6ca3d97120fda       storage-provisioner
c4290bb19da13       b79c189b052cd       About a minute ago   Running             kube-proxy                0                   75150e7c9fb0e       kube-proxy-c4d4r
5824183d55bd3       1cf5f116067c6       About a minute ago   Running             coredns                   0                   4b5d4fb16b6f8       coredns-674b8bbfcf-7vhx6
a4fe10f7da4c0       398c985c0d950       About a minute ago   Running             kube-scheduler            0                   d671ee408fdb9       kube-scheduler-minikube
a8b40beec7b17       c6ab243b29f82       About a minute ago   Running             kube-apiserver            0                   0e23f9d0e9872       kube-apiserver-minikube
21ee7ef01b31e       499038711c081       About a minute ago   Running             etcd                      0                   0de97b1e97478       etcd-minikube
626560403c0d7       ef43894fa110c       About a minute ago   Running             kube-controller-manager   0                   6d0c140d82170       kube-controller-manager-minikube


==> coredns [5824183d55bd] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:44416 - 26467 "HINFO IN 2125577843011873390.676005677678857280. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.01651211s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_23T12_37_31_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 23 Jul 2025 12:37:28 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 23 Jul 2025 12:38:42 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 23 Jul 2025 12:37:41 +0000   Wed, 23 Jul 2025 12:37:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 23 Jul 2025 12:37:41 +0000   Wed, 23 Jul 2025 12:37:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 23 Jul 2025 12:37:41 +0000   Wed, 23 Jul 2025 12:37:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 23 Jul 2025 12:37:41 +0000   Wed, 23 Jul 2025 12:37:28 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3926240Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3926240Ki
  pods:               110
System Info:
  Machine ID:                 a5a735c487d8440ab534ff35967437c2
  System UUID:                a5a735c487d8440ab534ff35967437c2
  Boot ID:                    2fc32cb7-f739-4651-898b-86fcf535cf82
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     flask-app-deployment-f9887c77d-d2b7f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7s
  kube-system                 coredns-674b8bbfcf-7vhx6                100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     66s
  kube-system                 etcd-minikube                           100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         71s
  kube-system                 kube-apiserver-minikube                 250m (3%)     0 (0%)      0 (0%)           0 (0%)         71s
  kube-system                 kube-controller-manager-minikube        200m (2%)     0 (0%)      0 (0%)           0 (0%)         71s
  kube-system                 kube-proxy-c4d4r                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         68s
  kube-system                 kube-scheduler-minikube                 100m (1%)     0 (0%)      0 (0%)           0 (0%)         71s
  kube-system                 storage-provisioner                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         70s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 65s                kube-proxy       
  Normal  NodeHasSufficientMemory  78s (x8 over 78s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    78s (x8 over 78s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     78s (x7 over 78s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  78s                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 72s                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  72s                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  71s                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    71s                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     71s                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           68s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.177951] pulseaudio[254]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.181648] Failed to connect to bus: No such file or directory
[  +0.221729] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.009333] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.008317] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002803] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003996] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004017] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.007135] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.004144] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003227] Failed to connect to bus: No such file or directory
[  +0.001460] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.009610] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.258717] Failed to connect to bus: No such file or directory
[  +0.652295] WSL (217) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.142103] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.417028] systemd-journald[62]: File /var/log/journal/05139adb280b4474a0495befa57e3677/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.138842] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001515] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.025973] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001578] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003903] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.006739] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001131] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001270] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.011610] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.012599] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.008984] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004936] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.005750] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.367100] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.514646] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2119: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.124696] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.626525] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.028371] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.024309] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001255] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.014702] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.017761] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.005305] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.009790] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.004372] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.016476] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.042882] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.324387] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.010295] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.018049] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.162870] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.034131] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001896] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001600] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001573] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.058069] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.068064] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.040504] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.048537] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.063486] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.368653] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.914847] WSL (2 - init-systemd(Ubuntu)) ERROR: WaitForBootProcess:3497: /sbin/init failed to start within 10000ms
[Jul23 10:30] netlink: 'init': attribute type 4 has an invalid length.


==> etcd [21ee7ef01b31] <==
{"level":"warn","ts":"2025-07-23T12:37:25.436982Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-07-23T12:37:25.437165Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-07-23T12:37:25.437192Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-07-23T12:37:25.437295Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-07-23T12:37:25.437323Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-07-23T12:37:25.437420Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-23T12:37:25.438277Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-07-23T12:37:25.438951Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-07-23T12:37:25.446657Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"6.472763ms"}
{"level":"info","ts":"2025-07-23T12:37:25.463831Z","caller":"etcdserver/raft.go:506","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-07-23T12:37:25.463970Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-07-23T12:37:25.464174Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-07-23T12:37:25.464199Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-07-23T12:37:25.464214Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-07-23T12:37:25.464290Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-07-23T12:37:25.492800Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-07-23T12:37:25.502728Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-07-23T12:37:25.502795Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":0}
{"level":"info","ts":"2025-07-23T12:37:25.507913Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-07-23T12:37:25.513739Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-07-23T12:37:25.513979Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-07-23T12:37:25.514143Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-23T12:37:25.514710Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-23T12:37:25.514747Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-23T12:37:25.515069Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-23T12:37:25.517135Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-07-23T12:37:25.517309Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-07-23T12:37:25.517690Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-23T12:37:25.517788Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-23T12:37:25.517827Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-23T12:37:25.518160Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-07-23T12:37:25.518250Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-07-23T12:37:25.665731Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-07-23T12:37:25.665818Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-07-23T12:37:25.665840Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-07-23T12:37:25.665899Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-07-23T12:37:25.666064Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-07-23T12:37:25.666092Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-07-23T12:37:25.666111Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-07-23T12:37:25.669241Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-23T12:37:25.669289Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-23T12:37:25.669309Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-23T12:37:25.669425Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-23T12:37:25.669861Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-23T12:37:25.669926Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-23T12:37:25.670711Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-23T12:37:25.670737Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-23T12:37:25.672879Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-07-23T12:37:25.673359Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-23T12:37:25.673531Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-07-23T12:37:25.674346Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-23T12:37:25.675233Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}


==> kernel <==
 12:38:43 up  2:09,  0 users,  load average: 0.84, 0.80, 0.44
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [a8b40beec7b1] <==
I0723 12:37:28.022900       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0723 12:37:28.022975       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0723 12:37:28.022991       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0723 12:37:28.023019       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0723 12:37:28.023038       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0723 12:37:28.023137       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0723 12:37:28.023149       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0723 12:37:28.023731       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0723 12:37:28.023796       1 controller.go:90] Starting OpenAPI V3 controller
I0723 12:37:28.023816       1 naming_controller.go:299] Starting NamingConditionController
I0723 12:37:28.023828       1 establishing_controller.go:81] Starting EstablishingController
I0723 12:37:28.023880       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0723 12:37:28.023905       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0723 12:37:28.023930       1 crd_finalizer.go:269] Starting CRDFinalizer
E0723 12:37:28.114100       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0723 12:37:28.123210       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0723 12:37:28.123250       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0723 12:37:28.123295       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0723 12:37:28.123358       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0723 12:37:28.123307       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0723 12:37:28.123337       1 aggregator.go:171] initial CRD sync complete...
I0723 12:37:28.123392       1 autoregister_controller.go:144] Starting autoregister controller
I0723 12:37:28.123403       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0723 12:37:28.123410       1 cache.go:39] Caches are synced for autoregister controller
I0723 12:37:28.123342       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0723 12:37:28.123488       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0723 12:37:28.123523       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0723 12:37:28.123547       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0723 12:37:28.123556       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0723 12:37:28.123576       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0723 12:37:28.123712       1 cache.go:39] Caches are synced for LocalAvailability controller
I0723 12:37:28.139299       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0723 12:37:28.151857       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0723 12:37:28.151890       1 policy_source.go:240] refreshing policies
E0723 12:37:28.177267       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0723 12:37:28.225123       1 controller.go:667] quota admission added evaluator for: namespaces
I0723 12:37:28.235558       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0723 12:37:28.235649       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0723 12:37:28.253557       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0723 12:37:28.253951       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0723 12:37:28.316411       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0723 12:37:29.030420       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0723 12:37:29.035866       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0723 12:37:29.035896       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0723 12:37:29.891452       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0723 12:37:29.964301       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0723 12:37:30.067168       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0723 12:37:30.155438       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0723 12:37:30.168559       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0723 12:37:30.170131       1 controller.go:667] quota admission added evaluator for: endpoints
I0723 12:37:30.181454       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0723 12:37:30.995857       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0723 12:37:31.026421       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0723 12:37:31.034947       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0723 12:37:34.860862       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0723 12:37:35.564112       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0723 12:37:35.577578       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0723 12:37:35.910866       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0723 12:38:35.560692       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0723 12:38:35.562774       1 alloc.go:328] "allocated clusterIPs" service="default/flask-app-service" clusterIPs={"IPv4":"10.103.153.151"}


==> kube-controller-manager [626560403c0d] <==
I0723 12:37:34.557957       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-approving-controller" name="csrapproving"
I0723 12:37:34.557967       1 shared_informer.go:350] "Waiting for caches to sync" controller="certificate-csrapproving"
I0723 12:37:34.712432       1 range_allocator.go:112] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses" logger="node-ipam-controller"
I0723 12:37:34.712489       1 controllermanager.go:778] "Started controller" controller="node-ipam-controller"
I0723 12:37:34.712778       1 node_ipam_controller.go:141] "Starting ipam controller" logger="node-ipam-controller"
I0723 12:37:34.712807       1 shared_informer.go:350] "Waiting for caches to sync" controller="node"
I0723 12:37:34.715821       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0723 12:37:34.723933       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0723 12:37:34.723985       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0723 12:37:34.727540       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0723 12:37:34.727795       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0723 12:37:34.745401       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0723 12:37:34.751993       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0723 12:37:34.758660       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0723 12:37:34.760085       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0723 12:37:34.761618       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0723 12:37:34.763108       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0723 12:37:34.763185       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0723 12:37:34.763288       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0723 12:37:34.773855       1 shared_informer.go:357] "Caches are synced" controller="job"
I0723 12:37:34.778518       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0723 12:37:34.790895       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0723 12:37:34.798853       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0723 12:37:34.808706       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0723 12:37:34.811977       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0723 12:37:34.812066       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0723 12:37:34.812218       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0723 12:37:34.813354       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0723 12:37:34.813415       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0723 12:37:34.813419       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0723 12:37:34.813443       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0723 12:37:34.813487       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0723 12:37:34.813509       1 shared_informer.go:357] "Caches are synced" controller="node"
I0723 12:37:34.813609       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0723 12:37:34.813676       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0723 12:37:34.813683       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0723 12:37:34.813687       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0723 12:37:34.813948       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0723 12:37:34.815902       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0723 12:37:34.817048       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0723 12:37:34.817188       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0723 12:37:34.817319       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0723 12:37:34.817336       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0723 12:37:34.817362       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0723 12:37:34.823768       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0723 12:37:34.862031       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0723 12:37:34.909678       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0723 12:37:34.928952       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0723 12:37:34.963106       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0723 12:37:34.963268       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0723 12:37:35.013910       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0723 12:37:35.040804       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0723 12:37:35.061752       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0723 12:37:35.112555       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0723 12:37:35.113550       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0723 12:37:35.115976       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0723 12:37:35.528227       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0723 12:37:35.608269       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0723 12:37:35.608301       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0723 12:37:35.608309       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [c4290bb19da1] <==
I0723 12:37:37.678363       1 server_linux.go:63] "Using iptables proxy"
I0723 12:37:37.859001       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0723 12:37:37.859101       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0723 12:37:37.888705       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0723 12:37:37.888759       1 server_linux.go:145] "Using iptables Proxier"
I0723 12:37:37.895144       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0723 12:37:37.908238       1 server.go:516] "Version info" version="v1.33.1"
I0723 12:37:37.908298       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0723 12:37:37.914577       1 config.go:199] "Starting service config controller"
I0723 12:37:37.914619       1 config.go:105] "Starting endpoint slice config controller"
I0723 12:37:37.914773       1 config.go:440] "Starting serviceCIDR config controller"
I0723 12:37:37.914812       1 config.go:329] "Starting node config controller"
I0723 12:37:37.915595       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0723 12:37:37.915620       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0723 12:37:37.915620       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0723 12:37:37.915643       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0723 12:37:38.015812       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0723 12:37:38.015879       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0723 12:37:38.015906       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0723 12:37:38.015933       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [a4fe10f7da4c] <==
I0723 12:37:26.619589       1 serving.go:386] Generated self-signed cert in-memory
W0723 12:37:28.039005       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0723 12:37:28.039604       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0723 12:37:28.039644       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0723 12:37:28.039659       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0723 12:37:28.091977       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0723 12:37:28.092021       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0723 12:37:28.096046       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0723 12:37:28.096404       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0723 12:37:28.096729       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0723 12:37:28.098128       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0723 12:37:28.099526       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0723 12:37:28.099536       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0723 12:37:28.099543       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0723 12:37:28.099795       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0723 12:37:28.100114       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0723 12:37:28.100304       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0723 12:37:28.100320       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0723 12:37:28.100377       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0723 12:37:28.100403       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0723 12:37:28.100402       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0723 12:37:28.100460       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0723 12:37:28.100404       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0723 12:37:28.100506       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0723 12:37:28.100670       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0723 12:37:28.100698       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0723 12:37:28.100711       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0723 12:37:28.927008       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0723 12:37:28.939951       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0723 12:37:28.967535       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0723 12:37:28.969816       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0723 12:37:29.009308       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0723 12:37:29.093142       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0723 12:37:29.111002       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0723 12:37:29.161271       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0723 12:37:29.183634       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0723 12:37:29.194151       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0723 12:37:29.361267       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0723 12:37:29.383803       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
I0723 12:37:30.797275       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.075300    2503 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109224    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109274    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-data\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109296    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109309    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109324    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/feee622ba49882ef945e2406d3ba86df-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"feee622ba49882ef945e2406d3ba86df\") " pod="kube-system/kube-scheduler-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109342    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109354    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109370    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109383    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109394    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109408    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109446    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-certs\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109496    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109525    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.109551    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.898238    2503 apiserver.go:52] "Watching apiserver"
Jul 23 12:37:31 minikube kubelet[2503]: I0723 12:37:31.906132    2503 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Jul 23 12:37:32 minikube kubelet[2503]: I0723 12:37:32.005815    2503 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jul 23 12:37:32 minikube kubelet[2503]: I0723 12:37:32.008187    2503 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jul 23 12:37:32 minikube kubelet[2503]: I0723 12:37:32.009085    2503 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:32 minikube kubelet[2503]: I0723 12:37:32.009444    2503 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jul 23 12:37:32 minikube kubelet[2503]: E0723 12:37:32.034549    2503 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jul 23 12:37:32 minikube kubelet[2503]: E0723 12:37:32.035014    2503 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jul 23 12:37:32 minikube kubelet[2503]: E0723 12:37:32.035014    2503 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jul 23 12:37:32 minikube kubelet[2503]: E0723 12:37:32.035014    2503 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Jul 23 12:37:32 minikube kubelet[2503]: I0723 12:37:32.087958    2503 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.08793077 podStartE2EDuration="1.08793077s" podCreationTimestamp="2025-07-23 12:37:31 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-23 12:37:32.055631591 +0000 UTC m=+1.230265781" watchObservedRunningTime="2025-07-23 12:37:32.08793077 +0000 UTC m=+1.262564960"
Jul 23 12:37:32 minikube kubelet[2503]: I0723 12:37:32.106793    2503 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.10677146 podStartE2EDuration="1.10677146s" podCreationTimestamp="2025-07-23 12:37:31 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-23 12:37:32.106482528 +0000 UTC m=+1.281116817" watchObservedRunningTime="2025-07-23 12:37:32.10677146 +0000 UTC m=+1.281405650"
Jul 23 12:37:32 minikube kubelet[2503]: I0723 12:37:32.107035    2503 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.107022896 podStartE2EDuration="1.107022896s" podCreationTimestamp="2025-07-23 12:37:31 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-23 12:37:32.089059347 +0000 UTC m=+1.263693637" watchObservedRunningTime="2025-07-23 12:37:32.107022896 +0000 UTC m=+1.281657186"
Jul 23 12:37:32 minikube kubelet[2503]: I0723 12:37:32.129221    2503 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.129150953 podStartE2EDuration="1.129150953s" podCreationTimestamp="2025-07-23 12:37:31 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-23 12:37:32.125845386 +0000 UTC m=+1.300479675" watchObservedRunningTime="2025-07-23 12:37:32.129150953 +0000 UTC m=+1.303785143"
Jul 23 12:37:34 minikube kubelet[2503]: I0723 12:37:34.981479    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/fb757853-b652-4d7e-a022-87f17431df8c-xtables-lock\") pod \"kube-proxy-c4d4r\" (UID: \"fb757853-b652-4d7e-a022-87f17431df8c\") " pod="kube-system/kube-proxy-c4d4r"
Jul 23 12:37:34 minikube kubelet[2503]: I0723 12:37:34.981550    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/fb757853-b652-4d7e-a022-87f17431df8c-lib-modules\") pod \"kube-proxy-c4d4r\" (UID: \"fb757853-b652-4d7e-a022-87f17431df8c\") " pod="kube-system/kube-proxy-c4d4r"
Jul 23 12:37:34 minikube kubelet[2503]: I0723 12:37:34.981588    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/fb757853-b652-4d7e-a022-87f17431df8c-kube-proxy\") pod \"kube-proxy-c4d4r\" (UID: \"fb757853-b652-4d7e-a022-87f17431df8c\") " pod="kube-system/kube-proxy-c4d4r"
Jul 23 12:37:34 minikube kubelet[2503]: I0723 12:37:34.981612    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6fgpn\" (UniqueName: \"kubernetes.io/projected/fb757853-b652-4d7e-a022-87f17431df8c-kube-api-access-6fgpn\") pod \"kube-proxy-c4d4r\" (UID: \"fb757853-b652-4d7e-a022-87f17431df8c\") " pod="kube-system/kube-proxy-c4d4r"
Jul 23 12:37:35 minikube kubelet[2503]: I0723 12:37:35.082466    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/0acc64b0-8ae2-4286-a207-31c5bcb0dd79-tmp\") pod \"storage-provisioner\" (UID: \"0acc64b0-8ae2-4286-a207-31c5bcb0dd79\") " pod="kube-system/storage-provisioner"
Jul 23 12:37:35 minikube kubelet[2503]: I0723 12:37:35.082537    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9lnc2\" (UniqueName: \"kubernetes.io/projected/0acc64b0-8ae2-4286-a207-31c5bcb0dd79-kube-api-access-9lnc2\") pod \"storage-provisioner\" (UID: \"0acc64b0-8ae2-4286-a207-31c5bcb0dd79\") " pod="kube-system/storage-provisioner"
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.087315    2503 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.087360    2503 projected.go:194] Error preparing data for projected volume kube-api-access-6fgpn for pod kube-system/kube-proxy-c4d4r: configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.087464    2503 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/fb757853-b652-4d7e-a022-87f17431df8c-kube-api-access-6fgpn podName:fb757853-b652-4d7e-a022-87f17431df8c nodeName:}" failed. No retries permitted until 2025-07-23 12:37:35.587438854 +0000 UTC m=+4.718220479 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-6fgpn" (UniqueName: "kubernetes.io/projected/fb757853-b652-4d7e-a022-87f17431df8c-kube-api-access-6fgpn") pod "kube-proxy-c4d4r" (UID: "fb757853-b652-4d7e-a022-87f17431df8c") : configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.187987    2503 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.188031    2503 projected.go:194] Error preparing data for projected volume kube-api-access-9lnc2 for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.188100    2503 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/0acc64b0-8ae2-4286-a207-31c5bcb0dd79-kube-api-access-9lnc2 podName:0acc64b0-8ae2-4286-a207-31c5bcb0dd79 nodeName:}" failed. No retries permitted until 2025-07-23 12:37:35.688081834 +0000 UTC m=+4.818863559 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-9lnc2" (UniqueName: "kubernetes.io/projected/0acc64b0-8ae2-4286-a207-31c5bcb0dd79-kube-api-access-9lnc2") pod "storage-provisioner" (UID: "0acc64b0-8ae2-4286-a207-31c5bcb0dd79") : configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.687067    2503 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.687140    2503 projected.go:194] Error preparing data for projected volume kube-api-access-6fgpn for pod kube-system/kube-proxy-c4d4r: configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.687247    2503 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/fb757853-b652-4d7e-a022-87f17431df8c-kube-api-access-6fgpn podName:fb757853-b652-4d7e-a022-87f17431df8c nodeName:}" failed. No retries permitted until 2025-07-23 12:37:36.687224819 +0000 UTC m=+5.818006444 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-6fgpn" (UniqueName: "kubernetes.io/projected/fb757853-b652-4d7e-a022-87f17431df8c-kube-api-access-6fgpn") pod "kube-proxy-c4d4r" (UID: "fb757853-b652-4d7e-a022-87f17431df8c") : configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.788273    2503 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.788322    2503 projected.go:194] Error preparing data for projected volume kube-api-access-9lnc2 for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Jul 23 12:37:35 minikube kubelet[2503]: E0723 12:37:35.788383    2503 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/0acc64b0-8ae2-4286-a207-31c5bcb0dd79-kube-api-access-9lnc2 podName:0acc64b0-8ae2-4286-a207-31c5bcb0dd79 nodeName:}" failed. No retries permitted until 2025-07-23 12:37:36.788370091 +0000 UTC m=+5.919151716 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-9lnc2" (UniqueName: "kubernetes.io/projected/0acc64b0-8ae2-4286-a207-31c5bcb0dd79-kube-api-access-9lnc2") pod "storage-provisioner" (UID: "0acc64b0-8ae2-4286-a207-31c5bcb0dd79") : configmap "kube-root-ca.crt" not found
Jul 23 12:37:36 minikube kubelet[2503]: I0723 12:37:36.090739    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fb44d\" (UniqueName: \"kubernetes.io/projected/4b22d8d2-dae3-4d2e-aaf7-ee3fe1fae4d4-kube-api-access-fb44d\") pod \"coredns-674b8bbfcf-7vhx6\" (UID: \"4b22d8d2-dae3-4d2e-aaf7-ee3fe1fae4d4\") " pod="kube-system/coredns-674b8bbfcf-7vhx6"
Jul 23 12:37:36 minikube kubelet[2503]: I0723 12:37:36.090825    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/4b22d8d2-dae3-4d2e-aaf7-ee3fe1fae4d4-config-volume\") pod \"coredns-674b8bbfcf-7vhx6\" (UID: \"4b22d8d2-dae3-4d2e-aaf7-ee3fe1fae4d4\") " pod="kube-system/coredns-674b8bbfcf-7vhx6"
Jul 23 12:37:37 minikube kubelet[2503]: I0723 12:37:37.182261    2503 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6ca3d97120fda4563552d74b2b1c7304cb503fd338f96eff9c28e0b657243f4b"
Jul 23 12:37:37 minikube kubelet[2503]: I0723 12:37:37.186179    2503 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="75150e7c9fb0e5a1e5a8d8657c27bb301f10787b620ef7d811d98e91565208d1"
Jul 23 12:37:38 minikube kubelet[2503]: I0723 12:37:38.225818    2503 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-7vhx6" podStartSLOduration=2.225765114 podStartE2EDuration="2.225765114s" podCreationTimestamp="2025-07-23 12:37:36 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-23 12:37:37.218579242 +0000 UTC m=+6.349360867" watchObservedRunningTime="2025-07-23 12:37:38.225765114 +0000 UTC m=+7.356546739"
Jul 23 12:37:38 minikube kubelet[2503]: I0723 12:37:38.242411    2503 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-c4d4r" podStartSLOduration=4.242385055 podStartE2EDuration="4.242385055s" podCreationTimestamp="2025-07-23 12:37:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-23 12:37:38.225700233 +0000 UTC m=+7.356481858" watchObservedRunningTime="2025-07-23 12:37:38.242385055 +0000 UTC m=+7.373166680"
Jul 23 12:37:38 minikube kubelet[2503]: I0723 12:37:38.242632    2503 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=6.242624222 podStartE2EDuration="6.242624222s" podCreationTimestamp="2025-07-23 12:37:32 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-23 12:37:38.242611186 +0000 UTC m=+7.373392811" watchObservedRunningTime="2025-07-23 12:37:38.242624222 +0000 UTC m=+7.373405847"
Jul 23 12:37:39 minikube kubelet[2503]: I0723 12:37:39.214762    2503 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jul 23 12:37:41 minikube kubelet[2503]: I0723 12:37:41.347095    2503 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jul 23 12:37:41 minikube kubelet[2503]: I0723 12:37:41.347892    2503 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jul 23 12:38:08 minikube kubelet[2503]: I0723 12:38:08.407921    2503 scope.go:117] "RemoveContainer" containerID="b2ce48c5b6fb2a837bf36c7539a176fc364010fbad9b53523086b1dda5e2ea9d"
Jul 23 12:38:35 minikube kubelet[2503]: I0723 12:38:35.544373    2503 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cjw2c\" (UniqueName: \"kubernetes.io/projected/5bb73833-5cf4-4cc1-8698-81d58b5de75a-kube-api-access-cjw2c\") pod \"flask-app-deployment-f9887c77d-d2b7f\" (UID: \"5bb73833-5cf4-4cc1-8698-81d58b5de75a\") " pod="default/flask-app-deployment-f9887c77d-d2b7f"


==> storage-provisioner [a71828e80f37] <==
I0723 12:38:08.626513       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0723 12:38:08.643613       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0723 12:38:08.645075       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
W0723 12:38:08.647090       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:08.659247       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
I0723 12:38:08.659976       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0723 12:38:08.660069       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"8629d51a-5d9e-40ca-84a6-e9544446820c", APIVersion:"v1", ResourceVersion:"434", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b6aa2fec-475e-4fac-83c2-c13f81e2f629 became leader
I0723 12:38:08.660161       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b6aa2fec-475e-4fac-83c2-c13f81e2f629!
W0723 12:38:08.662216       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:08.666638       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
I0723 12:38:08.761118       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b6aa2fec-475e-4fac-83c2-c13f81e2f629!
W0723 12:38:10.669620       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:10.675641       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:12.678849       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:12.684275       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:14.687425       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:14.700670       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:16.702886       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:16.708763       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:18.711631       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:18.717091       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:20.719632       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:20.728237       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:22.732055       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:22.737646       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:24.739525       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:24.745113       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:26.747180       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:26.757114       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:28.760076       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:28.765897       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:30.769369       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:30.775648       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:32.827689       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:32.836209       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:34.838893       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:34.844307       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:36.847655       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:36.853168       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:38.855588       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:38.864633       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:40.866742       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:40.872117       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:42.875628       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0723 12:38:42.881785       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [b2ce48c5b6fb] <==
I0723 12:37:37.551354       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0723 12:38:07.609982       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

